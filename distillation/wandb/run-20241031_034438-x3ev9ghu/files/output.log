/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
processing ./outputs/CounterCoTQA/gpt-neox-20b/train.modified.explanation.jsonl: 0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/yh158/mikd/cotkd_mi/distillation/main.py", line 525, in <module>
    main(args, args.seed)
  File "/home/yh158/mikd/cotkd_mi/distillation/main.py", line 236, in main
    trainset = get_tensor_dataset('train', tokenizer, args)
  File "/home/yh158/mikd/cotkd_mi/distillation/data_helper.py", line 191, in get_tensor_dataset
    inputs = tokenizer(input_seq, padding='max_length',
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3055, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3163, in _call_one
    return self.encode_plus(
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3228, in encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2959, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Traceback (most recent call last):
  File "/home/yh158/mikd/cotkd_mi/distillation/main.py", line 525, in <module>
    main(args, args.seed)
  File "/home/yh158/mikd/cotkd_mi/distillation/main.py", line 236, in main
    trainset = get_tensor_dataset('train', tokenizer, args)
  File "/home/yh158/mikd/cotkd_mi/distillation/data_helper.py", line 191, in get_tensor_dataset
    inputs = tokenizer(input_seq, padding='max_length',
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3055, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3163, in _call_one
    return self.encode_plus(
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3228, in encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/yh158/.conda/envs/mikd/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2959, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
